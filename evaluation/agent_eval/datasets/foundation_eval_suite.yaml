version: "1"
name: "foundation-offline-layered-suite"
description: "Hard-only offline foundation evaluation focused on challenging LLM-required reasoning and industry benchmark transfer: SWE-bench Verified readiness, conflict-heavy orchestration and intent decomposition, architecture coding hard tasks, artifact/reproducibility/stateful stress, and expanded benchmark-transfer sets (coding/workflow, web/computer-use, long-context reasoning, GAIA, LiveCodeBench/SWE-Lancer, AssistantBench/τ2, NoLiMa/LongMemEval/BABILong). Saturated easy/basic collections are retired from the main suite."
collections:
  - id: "swebench-verified-readiness"
    name: "SWE-bench Verified Readiness"
    dimension: "swebench_verified_readiness"
    description: "Routing readiness for SWE-bench Verified style software-fix tasks"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_swebench_verified_readiness.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 3

  - id: "multi-step-orchestration"
    name: "Multi-Step Orchestration"
    dimension: "multi_step_orchestration"
    description: "Systematic routing coverage for multi-step agent capability chains"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_multi_step_orchestration.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 3

  - id: "architecture-coding-hard"
    name: "Architecture Coding Hard"
    dimension: "architecture_coding_hard"
    description: "Routing quality for complex architecture refactors and logic-heavy coding tasks"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_architecture_coding_hard.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 3

  - id: "conflict-convergence-hard"
    name: "Conflict Convergence Hard"
    dimension: "conflict_convergence_hard"
    description: "Hard conflict-focused convergence set for overlapping tools and sandbox-intent normalization"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_conflict_convergence_hard.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 3

  - id: "intent-decomposition-constraint-matrix"
    name: "Intent Decomposition Constraint Matrix"
    dimension: "intent_decomposition_constraint_matrix"
    description: "Systematic matrix for intent decomposition under consent, memory, channel-action, retrieval, execution, scheduling, and deliverable constraints"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_intent_decomposition_constraint_matrix.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 3

  - id: "challenge-hard-v2"
    name: "Challenge Hard V2"
    dimension: "challenge_hard_v2"
    description: "High-ambiguity and conflict-heavy challenge set aligned with core goals"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_challenge_hard_v2.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "complex-artifact-delivery"
    name: "Complex Artifact Delivery"
    dimension: "complex_artifact_delivery"
    description: "Complex tasks that require concrete artifact/file outputs, traceability checks, and publish workflows"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_complex_artifact_delivery.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "sparse-clue-retrieval-stress"
    name: "Sparse-Clue Retrieval Stress"
    dimension: "sparse_clue_retrieval_stress"
    description: "BrowseComp-like sparse clue retrieval and source/memory/repo disambiguation stress"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_sparse_clue_retrieval.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "stateful-commitment-boundary-stress"
    name: "Stateful Commitment Boundary Stress"
    dimension: "stateful_commitment_boundary_stress"
    description: "TAU-bench-like stateful multi-turn commitment and mutation-boundary disambiguation stress"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_stateful_commitment_boundary.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "reproducibility-trace-evidence-stress"
    name: "Reproducibility Trace Evidence Stress"
    dimension: "reproducibility_trace_evidence_stress"
    description: "WebArena-Verified/SWE-Verified style reproducibility, traceability, and publish-gate routing stress"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_reproducibility_trace_evidence.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-coding-workflow"
    name: "Industry Benchmark Coding Workflow"
    dimension: "industry_benchmark_coding_workflow"
    description: "Transfer set mapped from SWE-bench Verified, τ-bench, and AgentBench coding/workflow failure modes"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_coding_workflow.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-web-and-computer-use"
    name: "Industry Benchmark Web and Computer Use"
    dimension: "industry_benchmark_web_and_computer_use"
    description: "Transfer set mapped from WebArena, BrowseComp, OSWorld, and WebChoreArena"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_web_and_computer_use.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-long-context-reasoning"
    name: "Industry Benchmark Long-Context Reasoning"
    dimension: "industry_benchmark_long_context_reasoning"
    description: "Transfer set mapped from LongBench v2, RULER, and InfiniteBench-style long-context reasoning stress"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_long_context_reasoning.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-general-assistant-gaia"
    name: "Industry Benchmark General Assistant GAIA"
    dimension: "industry_benchmark_general_assistant_gaia"
    description: "Transfer set mapped from GAIA-style general assistant tasks with gated actions and tool-use disambiguation"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_general_assistant_gaia.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-real-world-coding-livecodebench-swelancer"
    name: "Industry Benchmark Real-World Coding LiveCodeBench SWE-Lancer"
    dimension: "industry_benchmark_real_world_coding_livecodebench_swelancer"
    description: "Transfer set mapped from LiveCodeBench and SWE-Lancer real-world coding failure modes"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_real_world_coding_livecodebench_swelancer.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-multiturn-enterprise-assistantbench-tau2"
    name: "Industry Benchmark Multiturn Enterprise AssistantBench tau2"
    dimension: "industry_benchmark_multiturn_enterprise_assistantbench_tau2"
    description: "Transfer set mapped from AssistantBench and tau2-style stateful enterprise workflows"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_multiturn_enterprise_assistantbench_tau2.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5

  - id: "industry-benchmark-context-learning-nolima-longmemeval-babilong"
    name: "Industry Benchmark Context Learning NoLiMa LongMemEval BABILong"
    dimension: "industry_benchmark_context_learning_nolima_longmemeval_babilong"
    description: "Transfer set mapped from NoLiMa, LongMemEval, and BABILong context-learning failure modes"
    cases_path: "evaluation/agent_eval/datasets/foundation_eval_cases_industry_benchmark_context_learning_nolima_longmemeval_babilong.yaml"
    mode: "web"
    preset: "full"
    toolset: "default"
    top_k: 5
