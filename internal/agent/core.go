package agent

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"alex/internal/context/message"
	"alex/internal/llm"
	"alex/internal/session"
	"alex/internal/utils"
	"alex/pkg/types"
)

// ReactCore - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ReactCoreæ ¸å¿ƒå®ç°
type ReactCore struct {
	agent            *ReactAgent
	streamCallback   StreamCallback
	messageProcessor *message.MessageProcessor
	llmHandler       *LLMHandler
	toolHandler      *ToolHandler
	promptHandler    *PromptHandler
}

// NewReactCore - åˆ›å»ºReActæ ¸å¿ƒå®ä¾‹
func NewReactCore(agent *ReactAgent) *ReactCore {
	llmClient, err := llm.GetLLMInstance(llm.BasicModel)
	if err != nil {
		log.Printf("[ERROR] NewReactCore: Failed to get LLM instance: %v", err)
		llmClient = nil
	}

	return &ReactCore{
		agent:            agent,
		messageProcessor: message.NewMessageProcessor(llmClient, agent.sessionManager),
		llmHandler:       NewLLMHandler(agent.sessionManager, nil), // Will be set per request
		toolHandler:      NewToolHandler(agent.tools),
		promptHandler:    NewPromptHandler(agent.promptBuilder),
	}
}

// SolveTask - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ç®€åŒ–ä»»åŠ¡è§£å†³æ–¹æ³•
func (rc *ReactCore) SolveTask(ctx context.Context, task string, streamCallback StreamCallback) (*types.ReactTaskResult, error) {
	// Get session ID from context - unified approach
	// è®¾ç½®æµå›è°ƒ
	rc.streamCallback = streamCallback
	rc.llmHandler.streamCallback = streamCallback

	// ç”Ÿæˆä»»åŠ¡ID
	taskID := generateTaskID()

	// åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
	taskCtx := types.NewReactTaskContext(taskID, task)
	ctx = context.WithValue(ctx, utils.WorkingDirKey, taskCtx.WorkingDir)
	// å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
	isStreaming := streamCallback != nil
	if isStreaming {
		streamCallback(StreamChunk{Type: "status", Content: message.GetRandomProcessingMessage(), Metadata: map[string]any{"phase": "initialization"}})
	}

	// æ„å»ºç³»ç»Ÿæç¤ºï¼ˆåªéœ€æ„å»ºä¸€æ¬¡ï¼‰
	systemPrompt := rc.promptHandler.buildToolDrivenTaskPrompt(taskCtx)
	messages := []llm.Message{
		{
			Role:    "system",
			Content: systemPrompt,
		},
	}

	// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
	userMsg := &session.Message{
		Role:    "user",
		Content: task,
		Metadata: map[string]interface{}{
			"timestamp": time.Now().Unix(),
			"streaming": true,
		},
		Timestamp: time.Now(),
	}
	rc.agent.currentSession.AddMessage(userMsg)

	// æ‰§è¡Œå·¥å…·é©±åŠ¨çš„ReActå¾ªç¯
	maxIterations := 100 // å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œä¾èµ–æ™ºèƒ½å·¥å…·è°ƒç”¨

	for iteration := 1; iteration <= maxIterations; iteration++ {
		step := types.ReactExecutionStep{
			Number:    iteration,
			Timestamp: time.Now(),
		}

		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "iteration",
				Content:  fmt.Sprintf("ğŸ”„ Iteration %d: Processing with tool-driven approach...", iteration),
				Metadata: map[string]any{"iteration": iteration, "phase": "tool_driven_processing"}})
		}

		// ç¬¬ä¸€æ¬¡è¿­ä»£æ›´æ–°æ¶ˆæ¯åˆ—è¡¨ï¼Œæ·»åŠ æœ€æ–°çš„ä¼šè¯å†…å®¹
		if iteration == 1 {
			sess := rc.agent.currentSession
			// ä½¿ç”¨æ–°çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼˜åŒ–æ¶ˆæ¯
			sessionMessages := sess.GetMessages()

			// ä½¿ç”¨ç»Ÿä¸€æ¶ˆæ¯ç³»ç»Ÿè¿›è¡Œè½¬æ¢
			unifiedMessages := rc.messageProcessor.ConvertSessionToUnified(sessionMessages)
			llmMessages := rc.messageProcessor.ConvertUnifiedToLLM(unifiedMessages)
			messages = append(messages, llmMessages...)
		} else {
			// ä½¿ç”¨AIç»¼åˆå‹ç¼©ç³»ç»Ÿè¿›è¡Œå‹ç¼©
			unifiedMessages := rc.messageProcessor.ConvertLLMToUnified(messages)
			sessionMessages := rc.messageProcessor.ConvertUnifiedToSession(unifiedMessages)
			compressedSessionMessages := rc.messageProcessor.CompressMessages(ctx, sessionMessages)
			compressedUnified := rc.messageProcessor.ConvertSessionToUnified(compressedSessionMessages)
			messages = rc.messageProcessor.ConvertUnifiedToLLM(compressedUnified)
		}
		// æ„å»ºå¯ç”¨å·¥å…·åˆ—è¡¨ - æ¯è½®éƒ½åŒ…å«å·¥å…·å®šä¹‰ä»¥ç¡®ä¿æ¨¡å‹èƒ½è°ƒç”¨å·¥å…·
		tools := rc.toolHandler.buildToolDefinitions()

		request := &llm.ChatRequest{
			Messages:   messages,
			ModelType:  llm.BasicModel,
			Tools:      tools,
			ToolChoice: "auto",
			Config:     rc.agent.llmConfig,
			MaxTokens:  rc.agent.llmConfig.MaxTokens,
		}
		// è·å–LLMå®ä¾‹
		client, err := llm.GetLLMInstance(llm.BasicModel)
		if err != nil {
			log.Printf("[ERROR] ReactCore: Failed to get LLM instance at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM initialization failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM initialization failed at iteration %d: %w", iteration, err)
		}

		// æ·»åŠ è¯·æ±‚å‚æ•°éªŒè¯
		if err := rc.llmHandler.validateLLMRequest(request); err != nil {
			log.Printf("[ERROR] ReactCore: Invalid LLM request at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Invalid request: %v", err)})
			}
			return nil, fmt.Errorf("invalid LLM request at iteration %d: %w", iteration, err)
		}

		// æ‰§è¡ŒLLMè°ƒç”¨ï¼Œå¸¦é‡è¯•æœºåˆ¶
		response, err := rc.llmHandler.callLLMWithRetry(ctx, client, request, 3)
		if err != nil {
			log.Printf("[ERROR] ReactCore: LLM call failed at iteration %d after retries: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM call failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM call failed at iteration %d: %w", iteration, err)
		}

		// å¢å¼ºçš„å“åº”éªŒè¯
		if response == nil {
			err := fmt.Errorf("received nil response from LLM at iteration %d", iteration)
			log.Printf("[ERROR] ReactCore: %v", err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ Received empty response from LLM"})
			}
			return nil, err
		}

		if len(response.Choices) == 0 {
			log.Printf("[ERROR] ReactCore: No response choices at iteration %d", iteration)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ No response choices from LLM - API response format issue"})
			}
			return nil, fmt.Errorf("no response choices received at iteration %d - API response format issue", iteration)
		}

		choice := response.Choices[0]
		step.Thought = strings.TrimSpace(choice.Message.Content)

		// Extract token usage from response using compatible method
		usage := response.GetUsage()
		tokensUsed := usage.GetTotalTokens()
		promptTokens := usage.GetPromptTokens()
		completionTokens := usage.GetCompletionTokens()

		// Update task context with token usage
		taskCtx.TokensUsed += tokensUsed
		taskCtx.PromptTokens += promptTokens
		taskCtx.CompletionTokens += completionTokens
		step.TokensUsed = tokensUsed

		// Send token usage via stream callback
		if isStreaming && tokensUsed > 0 {
			streamCallback(StreamChunk{
				Type:             "token_usage",
				Content:          fmt.Sprintf("Tokens used: %d (prompt: %d, completion: %d)", tokensUsed, promptTokens, completionTokens),
				TokensUsed:       tokensUsed,
				TotalTokensUsed:  taskCtx.TokensUsed,
				PromptTokens:     promptTokens,
				CompletionTokens: completionTokens,
				Metadata:         map[string]any{"iteration": iteration, "phase": "token_accounting"},
			})
		}

		if len(choice.Message.Content) > 0 && len(choice.Message.ToolCalls) > 0 {
			streamCallback(StreamChunk{
				Type:     "thinking_result",
				Content:  choice.Message.Content,
				Metadata: map[string]any{"iteration": iteration, "phase": "thinking_result"}})
		}
		// æ·»åŠ assistantæ¶ˆæ¯åˆ°å¯¹è¯å†å²å’Œsession
		// é‡è¦ä¿®å¤ï¼šå³ä½¿æ²¡æœ‰contentï¼Œä¹Ÿè¦æ·»åŠ åŒ…å«å·¥å…·è°ƒç”¨çš„assistantæ¶ˆæ¯
		if len(choice.Message.Content) > 0 || len(choice.Message.ToolCalls) > 0 {
			log.Printf("[DEBUG] ReactCore: Adding assistant message - Content length: %d, ToolCalls: %d", len(choice.Message.Content), len(choice.Message.ToolCalls))
			messages = append(messages, choice.Message)
			// åŒæ—¶æ·»åŠ åˆ°sessionä»¥ä¾›memoryç³»ç»Ÿå­¦ä¹ 
			rc.addMessageToSession(&choice.Message)
		}

		// è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
		toolCalls := rc.agent.parseToolCalls(&choice.Message)
		log.Printf("[DEBUG] ReactCore: Parsed %d tool calls", len(toolCalls))

		// è®°å½•æ‰€æœ‰ä»LLMæ¥æ”¶åˆ°çš„å·¥å…·è°ƒç”¨IDï¼Œç”¨äºéªŒè¯å“åº”å®Œæ•´æ€§
		expectedToolCallIDs := make([]string, 0, len(choice.Message.ToolCalls))
		for _, tc := range choice.Message.ToolCalls {
			expectedToolCallIDs = append(expectedToolCallIDs, tc.ID)
			log.Printf("[DEBUG] ReactCore: Expected tool call ID: %s", tc.ID)
		}

		if len(toolCalls) > 0 {
			step.Action = "tool_execution"
			step.ToolCall = toolCalls // è®°å½•æ‰€æœ‰å·¥å…·è°ƒç”¨

			// æ‰§è¡Œå·¥å…·è°ƒç”¨
			toolResult := rc.agent.executeSerialToolsStream(ctx, toolCalls, streamCallback)
			step.Result = toolResult

			log.Printf("[DEBUG] ReactCore: Tool execution returned %d results", len(toolResult))
			for i, result := range toolResult {
				log.Printf("[DEBUG] ReactCore: Tool result %d - Tool: '%s', CallID: '%s', Success: %v", i, result.ToolName, result.CallID, result.Success)
			}

			// å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²å’Œsession
			if toolResult != nil {
				isGemini := strings.Contains(request.Config.BaseURL, "googleapis")
				log.Printf("[DEBUG] ReactCore: Building tool messages, isGemini: %v", isGemini)
				toolMessages := rc.toolHandler.buildToolMessages(toolResult, isGemini)
				log.Printf("[DEBUG] ReactCore: Built %d tool messages", len(toolMessages))

				for i, msg := range toolMessages {
					log.Printf("[DEBUG] ReactCore: Tool message %d - Role: '%s', ToolCallId: '%s'", i, msg.Role, msg.ToolCallId)
				}

				// éªŒè¯å“åº”å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªæœŸæœ›çš„å·¥å…·è°ƒç”¨IDéƒ½æœ‰å¯¹åº”çš„å“åº”
				receivedIDs := make(map[string]bool)
				for _, msg := range toolMessages {
					if msg.ToolCallId != "" {
						receivedIDs[msg.ToolCallId] = true
					}
				}

				// æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„å“åº”
				var missingIDs []string
				for _, expectedID := range expectedToolCallIDs {
					if !receivedIDs[expectedID] {
						missingIDs = append(missingIDs, expectedID)
					}
				}

				// å¦‚æœæœ‰ç¼ºå¤±çš„IDï¼Œç”Ÿæˆfallbackå“åº” - åŠ å¼ºé”™è¯¯å¤„ç†
				if len(missingIDs) > 0 {
					log.Printf("[ERROR] ReactCore: Missing responses for tool call IDs: %v", missingIDs)
					log.Printf("[ERROR] ReactCore: Expected IDs: %v, Received IDs: %v", expectedToolCallIDs, func() []string {
						var received []string
						for id := range receivedIDs {
							received = append(received, id)
						}
						return received
					}())

					for _, missingID := range missingIDs {
						// å°è¯•æ‰¾åˆ°å¯¹åº”çš„å·¥å…·åç§°
						var toolName = "unknown"
						for _, tc := range choice.Message.ToolCalls {
							if tc.ID == missingID {
								toolName = tc.Function.Name
								break
							}
						}

						fallbackMsg := llm.Message{
							Role:       "tool",
							Content:    fmt.Sprintf("Tool execution failed: no response generated for %s", toolName),
							ToolCallId: missingID,
							Name:       toolName,
						}
						toolMessages = append(toolMessages, fallbackMsg)
						log.Printf("[ERROR] ReactCore: Generated fallback response for missing ID: %s (tool: %s)", missingID, toolName)
					}

					// å¦‚æœæœ‰ç¼ºå¤±å“åº”ï¼Œé€šè¿‡æµå›è°ƒé€šçŸ¥ç”¨æˆ·
					if isStreaming {
						streamCallback(StreamChunk{
							Type:     "tool_error",
							Content:  fmt.Sprintf("Warning: %d tool call(s) failed to generate proper responses", len(missingIDs)),
							Metadata: map[string]any{"missing_tool_calls": missingIDs},
						})
					}
				}

				messages = append(messages, toolMessages...)

				// å°†å·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°sessionä¾›memoryç³»ç»Ÿå­¦ä¹ 
				rc.addToolMessagesToSession(toolMessages, toolResult)

				// è¯»å–å¹¶æ³¨å…¥å½“å‰TODOä½œä¸ºç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨å·¥å…·æ‰§è¡Œå®Œæˆåï¼‰
				if todoContent := rc.readCurrentTodos(ctx); todoContent != "" && !strings.Contains(todoContent, "No todo file found") {
					todoUserMessage := llm.Message{
						Role:    "user",
						Content: fmt.Sprintf("Current TODOs:\n%s", todoContent),
					}
					messages = append(messages, todoUserMessage)
					log.Printf("[DEBUG] ReactCore: Injected TODO message after tool execution")

					// æ·»åŠ åˆ°session
					rc.addUserMessageToSession(fmt.Sprintf("Current TODOs:\n%s", todoContent))
				}

				step.Observation = rc.toolHandler.generateObservation(toolResult)
			}
		} else {
			finalAnswer := choice.Message.Content

			step.Action = "direct_answer"
			step.Observation = finalAnswer
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			result := buildFinalResult(taskCtx, finalAnswer, 0.8, true)
			result.TokensUsed = taskCtx.TokensUsed

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  finalAnswer,
					Metadata: map[string]any{"iteration": iteration, "phase": "final_answer"}})
			}
			return result, nil
		}

		step.Duration = time.Since(step.Timestamp)
		taskCtx.History = append(taskCtx.History, step)
		taskCtx.LastUpdate = time.Now()
	}

	// è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
	log.Printf("[WARN] ReactCore: Maximum iterations (%d) reached", maxIterations)
	if isStreaming {
		streamCallback(StreamChunk{
			Type:     "max_iterations",
			Content:  fmt.Sprintf("âš ï¸ Reached maximum iterations (%d)", maxIterations),
			Metadata: map[string]any{"max_iterations": maxIterations}})
	}

	return buildFinalResult(taskCtx, "Maximum iterations reached without completion", 0.5, false), nil
}

// addMessageToSession - å°†LLMæ¶ˆæ¯æ·»åŠ åˆ°sessionä¸­ä¾›memoryç³»ç»Ÿå­¦ä¹ 
func (rc *ReactCore) addMessageToSession(llmMsg *llm.Message) {
	// è·å–å½“å‰ä¼šè¯
	sess := rc.agent.currentSession
	if sess == nil {
		return // æ²¡æœ‰ä¼šè¯åˆ™è·³è¿‡
	}

	// è½¬æ¢LLMæ¶ˆæ¯ä¸ºsessionæ¶ˆæ¯æ ¼å¼
	sessionMsg := &session.Message{
		Role:      llmMsg.Role,
		Content:   llmMsg.Content,
		Timestamp: time.Now(),
		Metadata: map[string]interface{}{
			"source":    "llm_response",
			"timestamp": time.Now().Unix(),
		},
	}

	// è½¬æ¢å·¥å…·è°ƒç”¨ä¿¡æ¯
	if len(llmMsg.ToolCalls) > 0 {
		for _, tc := range llmMsg.ToolCalls {
			// å°†Argumentså­—ç¬¦ä¸²è§£æä¸ºmap[string]interface{}
			var args map[string]interface{}
			if tc.Function.Arguments != "" {
				// ç®€å•å¤„ç†ï¼šå¦‚æœæ˜¯JSONå­—ç¬¦ä¸²å°è¯•è§£æï¼Œå¦åˆ™å­˜ä¸ºå­—ç¬¦ä¸²
				args = map[string]interface{}{"raw": tc.Function.Arguments}
			}

			sessionMsg.ToolCalls = append(sessionMsg.ToolCalls, session.ToolCall{
				ID:   tc.ID,
				Name: tc.Function.Name,
				Args: args,
			})
		}
		sessionMsg.Metadata["has_tool_calls"] = true
		sessionMsg.Metadata["tool_count"] = len(llmMsg.ToolCalls)
	}

	// æ·»åŠ åˆ°session
	sess.AddMessage(sessionMsg)
}

// addToolMessagesToSession - å°†å·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°sessionä¸­ä¾›memoryç³»ç»Ÿå­¦ä¹ 
func (rc *ReactCore) addToolMessagesToSession(toolMessages []llm.Message, toolResults []*types.ReactToolResult) {
	// è·å–å½“å‰ä¼šè¯
	sess := rc.agent.currentSession
	if sess == nil {
		return // æ²¡æœ‰ä¼šè¯åˆ™è·³è¿‡
	}

	// å¤„ç†æ¯ä¸ªå·¥å…·æ¶ˆæ¯
	for _, toolMsg := range toolMessages {
		sessionMsg := &session.Message{
			Role:      toolMsg.Role,
			Content:   toolMsg.Content,
			Timestamp: time.Now(),
			Metadata: map[string]interface{}{
				"source":    "tool_result",
				"timestamp": time.Now().Unix(),
			},
		}

		// ä¿å­˜tool_call_idåˆ°metadataä¸­ - è¿™æ˜¯å…³é”®ä¿®å¤
		if toolMsg.ToolCallId != "" {
			sessionMsg.Metadata["tool_call_id"] = toolMsg.ToolCallId
		}

		// å¦‚æœæ˜¯å·¥å…·ç»“æœæ¶ˆæ¯ï¼Œæ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
		if toolMsg.Role == "tool" && len(toolResults) > 0 {
			// å°è¯•åŒ¹é…å¯¹åº”çš„å·¥å…·ç»“æœ
			for _, result := range toolResults {
				if result != nil && toolMsg.ToolCallId == result.CallID {
					sessionMsg.Metadata["tool_name"] = result.ToolName
					sessionMsg.Metadata["tool_success"] = result.Success
					sessionMsg.Metadata["execution_time"] = result.Duration.Milliseconds()
					if result.Error != "" {
						sessionMsg.Metadata["tool_error"] = result.Error
					}
					break
				}
			}
		}

		// æ·»åŠ åˆ°session
		sess.AddMessage(sessionMsg)
	}

	// Memory creation removed
}

// readCurrentTodos - è¯»å–å½“å‰ä¼šè¯çš„TODOåˆ—è¡¨
func (rc *ReactCore) readCurrentTodos(ctx context.Context) string {
	// ç›´æ¥ä»agentè·å–session IDï¼Œé¿å…contextä¼ é€’çš„å¤æ‚æ€§
	if rc.agent.currentSession == nil {
		log.Printf("[DEBUG] ReactCore: No current session, cannot read todos")
		return ""
	}

	sessionID := rc.agent.currentSession.ID
	if sessionID == "" {
		log.Printf("[DEBUG] ReactCore: Current session has empty ID, cannot read todos")
		return ""
	}

	// ç›´æ¥è°ƒç”¨todoå·¥å…·ï¼Œä¼ é€’session IDä½œä¸ºå‚æ•°
	if todoTool, exists := rc.agent.tools["todo_read"]; exists {
		args := map[string]interface{}{
			"session_id": sessionID,
		}
		result, err := todoTool.Execute(ctx, args)
		if err != nil {
			log.Printf("[DEBUG] ReactCore: Failed to read todos: %v", err)
			return ""
		}
		if result != nil && result.Content != "" {
			return result.Content
		}
	}
	return ""
}

// addUserMessageToSession - å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°sessionä¸­
func (rc *ReactCore) addUserMessageToSession(content string) {
	// è·å–å½“å‰ä¼šè¯
	sess := rc.agent.currentSession
	if sess == nil {
		return // æ²¡æœ‰ä¼šè¯åˆ™è·³è¿‡
	}

	// åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
	sessionMsg := &session.Message{
		Role:      "user",
		Content:   content,
		Timestamp: time.Now(),
		Metadata: map[string]interface{}{
			"source":    "todo_injection",
			"timestamp": time.Now().Unix(),
		},
	}

	// æ·»åŠ åˆ°session
	sess.AddMessage(sessionMsg)
}
